# app/api/v1/analyze.py
from __future__ import annotations

import hashlib
import json
import logging

import duckdb
from fastapi import (
    APIRouter,
    BackgroundTasks,
    File,
    Form,
    HTTPException,
    Response,
    UploadFile,
)

from app.api.v1.jobs import _payload
from app.core import paths
from app.core.config import settings
from app.core.jobs_db import (
    compute_sha256,
    create_job,
    find_succeeded_by_sha,
    get_job,
    update_status,
)
from app.core.orchestrator import run as orchestrate_run
from app.criteria.protocols import Context
from app.criteria.runner import run_predicates  # used only for sync analyze()
from app.evidence.writer import mirror_jsonl_to_parquet

from .models import (
    AnalyzeContract,
    AnalyzeRequest,
    EvidenceItem,
    PredicateResult,
    Vendor,
)

router = APIRouter()
log = logging.getLogger("maturity.api.analyze")

# Guardrails:
# - Enforce max upload size early.
# - Ensure a stable model root exists before any per-model work.
MAX_UPLOAD_MB = settings.MAX_UPLOAD_MB

paths.MODELS_DIR.mkdir(parents=True, exist_ok=True)


def _coerce_maturity_level(level_obj) -> int:
    if isinstance(level_obj, int):
        return level_obj
    if isinstance(level_obj, (tuple, list)) and level_obj:
        return int(level_obj[0])
    if isinstance(level_obj, str):
        return int(level_obj.split("/", 1)[0])
    raise ValueError(f"unsupported maturity level type: {type(level_obj).__name__}")


# DEV-ONLY synchronous analysis:
# - One-shot ingest → IR → predicate run, returns results in the response.
# - UI should use /upload (async + job polling) for large models.
@router.post("", response_model=AnalyzeContract, response_model_exclude_none=True)
def analyze(req: AnalyzeRequest, response: Response) -> AnalyzeContract:
    """
    Synchronous analysis (dev-only). For normal UI, use /upload + job polling.
    """
    model_sha = compute_sha256(req.xml_bytes)
    # Content-addressable ID: stable across identical files; used for idempotency.
    model_id = req.model_id or model_sha[:8]
    model_dir = paths.model_dir(model_id)
    model_dir.mkdir(parents=True, exist_ok=True)

    xml_path = model_dir / "model.xml"
    if not xml_path.exists():
        xml_path.write_bytes(req.xml_bytes)

    con = None
    try:
        # Ingest only; skip RAG/predicates here to keep the sync path responsive.
        orchestrate_run(
            model_id=model_id,
            xml_path=xml_path,
            overwrite=True,
            build_rag=False,
            run_predicates=False,
        )

        # Open DuckDB and run predicates synchronously to return results
        # Open the per-model DuckDB generated by ingest;
        # use object cache for faster repeated queries.
        con = duckdb.connect(str(model_dir / "model.duckdb"))
        con.execute("PRAGMA enable_object_cache=true;")
        ctx = Context(
            vendor=req.vendor.value,
            version=req.version,
            model_dir=model_dir,
            model_id=model_id,
            output_root=paths.MODELS_DIR,
        )
        level, evidence, _levels = run_predicates(con, ctx)

        # Best-effort mirrors / RAG
        try:
            mirror_jsonl_to_parquet(model_dir)
        except Exception:
            log.debug("parquet_mirror_skipped model_id=%s", model_id, exc_info=True)
        try:
            orchestrate_run(
                model_id=model_id, xml_path=None, overwrite=False, build_rag=True
            )
        except Exception:
            log.warning("rag_bootstrap_failed model_id=%s", model_id, exc_info=True)

        # Flatten evidence into stable, API-facing results (typed, minimal, deterministic order).
        results = _normalize_results(evidence)
        passed = sum(1 for r in results if r.passed)
        failed = len(results) - passed

        # Analysis fingerprint:
        # - Hash of normalized results → lets clients detect "same analysis" across re-runs.
        fingerprint = hashlib.sha256(
            json.dumps(
                [
                    r.model_dump(exclude_none=True)
                    for r in sorted(results, key=lambda r: (r.mml, r.id))
                ],
                sort_keys=True,
                separators=(",", ":"),
            ).encode("utf-8")
        ).hexdigest()

        if response is not None:
            # Expose model hash + analysis fingerprint for caching and diffing in the UI.
            response.headers["X-Model-SHA256"] = model_sha
            response.headers["X-Analysis-Fingerprint"] = fingerprint

        return AnalyzeContract(
            model={"vendor": req.vendor.value, "version": req.version},
            maturity_level=_coerce_maturity_level(level),
            summary={"total": len(results), "passed": passed, "failed": failed},
            results=results,
        )

    # Input/validation errors → 400; unexpected failures → 500 with server-side logs.
    except ValueError as e:
        raise HTTPException(status_code=400, detail=str(e)) from e
    except Exception:
        log.exception("analysis_failed model_id=%s", model_id)
        raise HTTPException(status_code=500, detail="analysis_failed")
    finally:
        try:
            if con is not None:
                con.close()
        except Exception:
            log.warning("db_close_failed model_id=%s", model_id, exc_info=True)


# Async pipeline:
# - Accepts multipart upload; creates a job; runs the whole pipeline in the background.
# - Returns 202 + Location:/v1/jobs/{id} for polling.
@router.post("/upload", status_code=202)
async def analyze_upload(
    response: Response,
    background: BackgroundTasks,
    file: UploadFile = File(...),
    vendor: Vendor = Form(...),
    version: str = Form(...),
    model_id: str | None = Form(None),
):
    data = await file.read()
    # Hard reject oversize uploads at the edge (consistent with infrastructure limits).
    if len(data) > MAX_UPLOAD_MB * 1024 * 1024:
        raise HTTPException(status_code=413, detail="file_too_large")

    sha = compute_sha256(data)

    # Reuse completed result if the same (sha, vendor, version) already succeeded
    # Idempotency: if (sha,vendor,version) already succeeded, skip to that job/result.
    existing = find_succeeded_by_sha(sha, vendor.value, version)
    if existing and existing.get("status") == "succeeded":
        job_id = existing["id"]
        # Always read the canonical row so progress/message/timings/types are correct
        row = get_job(job_id)
        if not row:
            # Extremely rare: index said yes but row missing; synthesize a safe row
            row = {  # type: ignore[assignment]
                "id": job_id,
                "sha256": sha,
                "model_id": existing["model_id"],
                "vendor": vendor.value,
                "version": version,
                "status": "succeeded",
                "progress": 100,
                "created_at": 0,
                "updated_at": 0,
            }
        response.headers["Location"] = f"/v1/jobs/{job_id}"
        return _payload(row)

    # Fresh job
    mid = model_id or sha[:8]
    job_id = create_job(sha, mid, vendor.value, version)

    model_dir = paths.model_dir(mid)
    model_dir.mkdir(parents=True, exist_ok=True)
    (model_dir / "model.xml").write_bytes(data)

    # Kick off the pipeline in background
    background.add_task(_run_pipeline_job, job_id, mid)

    # Return a normalized snapshot (progress 0)
    row = get_job(job_id)
    if not row:
        # Shouldn't happen, but keep the shape perfect if it does
        row = {  # type: ignore[assignment]
            "id": job_id,
            "sha256": sha,
            "model_id": mid,
            "vendor": vendor.value,
            "version": version,
            "status": "queued",
            "progress": 0,
            "created_at": 0,
            "updated_at": 0,
        }
    response.headers["Location"] = f"/v1/jobs/{job_id}"
    return _payload(row)


# Background job:
# - Reads job context, runs orchestrator end-to-end (ingest → predicates → RAG).
# - Always update job status/progress; never raise past this boundary.
def _run_pipeline_job(job_id: str, model_id: str) -> None:
    try:
        update_status(job_id, "running", progress=10)
        xml_path = paths.model_dir(model_id) / "model.xml"

        # Guardrail: fail fast if the upload didn't persist the XML.
        if not xml_path.exists():
            update_status(
                job_id, "failed", progress=100, message=f"missing xml: {xml_path}"
            )
            return

        # Pull vendor/version from the job row to keep the run context consistent with the request.
        row = get_job(job_id) or {}
        vendor = row.get("vendor") or ""
        version = row.get("version") or ""

        # Single source of truth for pipeline steps;
        # overwrite=False preserves previous artifacts.
        orchestrate_run(
            model_id=model_id,
            xml_path=xml_path,
            overwrite=False,
            build_rag=True,
            run_predicates=True,
            vendor=vendor,
            version=version,
        )

        update_status(job_id, "succeeded", progress=100)
    except Exception as e:
        update_status(
            job_id, "failed", progress=100, message=f"{type(e).__name__}: {e}"
        )


# Convert raw EvidenceItem → PredicateResult for the API contract.
# - Parse MML tier from predicate id (e.g., "mml_1:..."); default to 0 if malformed.
# - Keep details compact and JSON-serializable.
def _normalize_results(evidence: list[EvidenceItem]) -> list[PredicateResult]:
    out: list[PredicateResult] = []
    for e in evidence:
        pid = e.predicate
        try:
            mml = int(pid.split(":")[0].split("_")[1])
        except Exception:
            mml = 0
        out.append(
            PredicateResult(
                id=pid,
                mml=mml,
                passed=bool(e.passed),
                details=dict(e.details),
                error=(str(e.error) if getattr(e, "error", None) else None),
            )
        )
    return out
