# app/api/v1/analyze.py
from __future__ import annotations

import hashlib
import json
import logging

import duckdb
from fastapi import APIRouter, HTTPException, UploadFile, File, Form, Response, BackgroundTasks

from app.core import paths
from app.core.config import settings
from app.core.orchestrator import run as orchestrate_run
from app.core.jobs_db import create_job, update_status, find_succeeded_by_sha, compute_sha256, get_job
from app.criteria.runner import run_predicates  # used only for sync analyze()
from app.criteria.protocols import Context
from app.evidence.writer import mirror_jsonl_to_parquet
from app.utils.timing import now_ns, ms_since
from .models import AnalyzeRequest, Vendor, AnalyzeContract, EvidenceItem, PredicateResult

router = APIRouter()
log = logging.getLogger("maturity.api.analyze")

# Guardrails:
# - Enforce max upload size early.
# - Ensure a stable model root exists before any per-model work.
MAX_UPLOAD_MB = settings.MAX_UPLOAD_MB

paths.MODELS_DIR.mkdir(parents=True, exist_ok=True)

# DEV-ONLY synchronous analysis:
# - One-shot ingest → IR → predicate run, returns results in the response.
# - UI should use /upload (async + job polling) for large models.
@router.post("", response_model=AnalyzeContract, response_model_exclude_none=True)
def analyze(req: AnalyzeRequest, response: Response) -> AnalyzeContract:
    """
    Synchronous analysis (dev-only). For normal UI, use /upload + job polling.
    """
    model_sha = compute_sha256(req.xml_bytes)
        # Content-addressable ID: stable across identical files; used for idempotency.
    model_id = req.model_id or model_sha[:8]
    model_dir = paths.model_dir(model_id)
    model_dir.mkdir(parents=True, exist_ok=True)

    xml_path = model_dir / "model.xml"
    if not xml_path.exists():
        xml_path.write_bytes(req.xml_bytes)

    con = None
    try:
        # Ingest only; skip RAG/predicates here to keep the sync path responsive.
        orchestrate_run(model_id=model_id, xml_path=xml_path, overwrite=True, build_rag=False, run_predicates=False)
        
        # Open DuckDB and run predicates synchronously to return results
        # Open the per-model DuckDB generated by ingest; 
        # use object cache for faster repeated queries.
        con = duckdb.connect(str(model_dir / "model.duckdb"))
        con.execute("PRAGMA enable_object_cache=true;")
        ctx = Context(
            vendor=req.vendor.value,
            version=req.version,
            model_dir=model_dir,
            model_id=model_id,
            output_root=paths.MODELS_DIR,
        )
        level, evidence = run_predicates(con, ctx)

        # Best-effort mirrors / RAG
        try:
            mirror_jsonl_to_parquet(model_dir)
        except Exception:
            log.debug("parquet_mirror_skipped model_id=%s", model_id, exc_info=True)
        try:
            orchestrate_run(model_id=model_id, xml_path=None, overwrite=False, build_rag=True)
        except Exception:
            log.warning("rag_bootstrap_failed model_id=%s", model_id, exc_info=True)
        
        # Flatten evidence into stable, API-facing results (typed, minimal, deterministic order).
        results = _normalize_results(evidence)
        passed = sum(1 for r in results if r.passed)
        failed = len(results) - passed
        
        # Analysis fingerprint:
        # - Hash of normalized results → lets clients detect "same analysis" across re-runs.
        fingerprint = hashlib.sha256(
            json.dumps(
                [r.model_dump(exclude_none=True) for r in sorted(results, key=lambda r: (r.mml, r.id))],
                sort_keys=True,
                separators=(",", ":"),
            ).encode("utf-8")
        ).hexdigest()

        if response is not None:
            # Expose model hash + analysis fingerprint for caching and diffing in the UI.
            response.headers["X-Model-SHA256"] = model_sha
            response.headers["X-Analysis-Fingerprint"] = fingerprint

        return AnalyzeContract(
            model={"vendor": req.vendor.value, "version": req.version},
            maturity_level=level,
            summary={"total": len(results), "passed": passed, "failed": failed},
            results=results,
        )
        
    # Input/validation errors → 400; unexpected failures → 500 with server-side logs.
    except ValueError as e:
        raise HTTPException(status_code=400, detail=str(e)) from e
    except Exception:
        log.exception("analysis_failed model_id=%s", model_id)
        raise HTTPException(status_code=500, detail="analysis_failed")
    finally:
        try:
            if con is not None:
                con.close()
        except Exception:
            log.warning("db_close_failed model_id=%s", model_id, exc_info=True)

# Async pipeline:
# - Accepts multipart upload; creates a job; runs the whole pipeline in the background.
# - Returns 202 + Location:/v1/jobs/{id} for polling.
@router.post("/upload", status_code=202)
async def analyze_upload(
    response: Response,
    background: BackgroundTasks,
    file: UploadFile = File(...),
    vendor: Vendor = Form(...),
    version: str = Form(...),
    model_id: str | None = Form(None),
):
    data = await file.read()
    # Hard reject oversize uploads at the edge (consistent with infrastructure limits).
    if len(data) > MAX_UPLOAD_MB * 1024 * 1024:
        raise HTTPException(status_code=413, detail="file_too_large")

    sha = compute_sha256(data)

    # Reuse completed result if the same (sha, vendor, version) already succeeded
    # Idempotency: if (sha,vendor,version) already succeeded, skip to that job/result.
    existing = find_succeeded_by_sha(sha, vendor.value, version)
    if existing and existing.get("status") == "succeeded":
        job_id = existing["id"]
        
        # Human-friendly short ID fallback (first 8 of sha) if caller didn't provide model_id.
        mid = existing["model_id"]
        response.headers["Location"] = f"/v1/jobs/{job_id}"
        return {
            "job_id": job_id,
            "model_id": mid,
            "status": "succeeded",
            "sha256": sha,
            "links": {"self": f"/v1/jobs/{job_id}", "result": f"/v1/models/{mid}"},
        }

    mid = (model_id or sha[:8])
    job_id = create_job(sha, mid, vendor.value, version)

    model_dir = paths.model_dir(mid)
    model_dir.mkdir(parents=True, exist_ok=True)
    (model_dir / "model.xml").write_bytes(data)

    # Run the single orchestrator in the background
    background.add_task(_run_pipeline_job, job_id, mid)

    response.headers["Location"] = f"/v1/jobs/{job_id}"
    return {
        "job_id": job_id,
        "model_id": mid,
        "status": "queued",
        "sha256": sha,
        "links": {"self": f"/v1/jobs/{job_id}", "result": f"/v1/models/{mid}"},
    }

# Background job:
# - Reads job context, runs orchestrator end-to-end (ingest → predicates → RAG).
# - Always update job status/progress; never raise past this boundary.
def _run_pipeline_job(job_id: str, model_id: str) -> None:
    try:
        update_status(job_id, "running", progress=10)
        xml_path = paths.model_dir(model_id) / "model.xml"
        
        # Guardrail: fail fast if the upload didn't persist the XML.
        if not xml_path.exists():
            update_status(job_id, "failed", progress=100, message=f"missing xml: {xml_path}")
            return
        
        # Pull vendor/version from the job row to keep the run context consistent with the request.
        row = get_job(job_id) or {}
        vendor = (row.get("vendor") or "")
        version = (row.get("version") or "")
        
        # Single source of truth for pipeline steps; 
        # overwrite=False preserves previous artifacts.
        orchestrate_run(model_id=model_id, xml_path=xml_path, overwrite=False, build_rag=True,
                        run_predicates=True, vendor=vendor, version=version)
        
        update_status(job_id, "succeeded", progress=100)
    except Exception as e:
        update_status(job_id, "failed", progress=100, message=f"{type(e).__name__}: {e}")

# Convert raw EvidenceItem → PredicateResult for the API contract.
# - Parse MML tier from predicate id (e.g., "mml_1:..."); default to 0 if malformed.
# - Keep details compact and JSON-serializable.
def _normalize_results(evidence: list[EvidenceItem]) -> list[PredicateResult]:
    out: list[PredicateResult] = []
    for e in evidence:
        pid = e.predicate
        try:
            mml = int(pid.split(":")[0].split("_")[1])
        except Exception:
            mml = 0
        out.append(
            PredicateResult(
                id=pid,
                mml=mml,
                passed=bool(e.passed),
                details=dict(e.details),
                error=(str(e.error) if getattr(e, "error", None) else None),
            )
        )
    return out
